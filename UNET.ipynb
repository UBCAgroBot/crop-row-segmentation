{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "8WwjSmDt5sPj"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import InterpolationMode\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wZJZ7YFP59Xs"
   },
   "source": [
    "JPEG Images are the base images and Segmentation Class has masked images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "NEZQLeWygXQ-"
   },
   "outputs": [],
   "source": [
    "#check with neil for pylance errors\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet architecture implementation from the original paper:\n",
    "    \"U-Net: Convolutional Networks for Biomedical Image Segmentation\"\n",
    "    input_channels:\n",
    "    num_classes: number of classes in pixels\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, input_channels, num_classes=2):\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        # Encoder (Contracting Path)\n",
    "        # Each encoder block: Conv -> ReLU -> Conv -> ReLU\n",
    "        #For example 572\n",
    "        self.enc1_conv1 = nn.Conv2d(input_channels, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.enc1_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.enc2_conv1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.enc2_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.enc3_conv1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.enc3_conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.enc4_conv1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.enc4_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck_conv1 = nn.Conv2d(512, 1024, kernel_size=3, padding=1)\n",
    "        self.bottleneck_conv2 = nn.Conv2d(1024, 1024, kernel_size=3, padding=1)\n",
    "\n",
    "        # Decoder (Expansive Path)\n",
    "        # Each decoder block: UpConv -> Concat -> Conv -> ReLU -> Conv -> ReLU\n",
    "        self.upconv4 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.dec4_conv1 = nn.Conv2d(1024, 512, kernel_size=3, padding=1)\n",
    "        self.dec4_conv2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv3 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec3_conv1 = nn.Conv2d(512, 256, kernel_size=3, padding=1)\n",
    "        self.dec3_conv2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv2 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec2_conv1 = nn.Conv2d(256, 128, kernel_size=3, padding=1)\n",
    "        self.dec2_conv2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "\n",
    "        self.upconv1 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec1_conv1 = nn.Conv2d(128, 64, kernel_size=3, padding=1)\n",
    "        self.dec1_conv2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "\n",
    "        # Final output layer\n",
    "        self.out_conv = nn.Conv2d(64, num_classes, kernel_size=1)\n",
    "        # Max pooling layer (shared across encoder blocks)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder Block 1\n",
    "        enc1 = F.relu(self.enc1_conv1(x))\n",
    "        enc1 = F.relu(self.enc1_conv2(enc1))\n",
    "        enc1_pool = self.pool(enc1)\n",
    "\n",
    "        # Encoder Block 2\n",
    "        enc2 = F.relu(self.enc2_conv1(enc1_pool))\n",
    "        enc2 = F.relu(self.enc2_conv2(enc2))\n",
    "        enc2_pool = self.pool(enc2)\n",
    "        \n",
    "        # Encoder Block 3\n",
    "        enc3 = F.relu(self.enc3_conv1(enc2_pool))\n",
    "        enc3 = F.relu(self.enc3_conv2(enc3))\n",
    "        enc3_pool = self.pool(enc3)\n",
    "\n",
    "        # Encoder Block 4\n",
    "        enc4 = F.relu(self.enc4_conv1(enc3_pool))\n",
    "        enc4 = F.relu(self.enc4_conv2(enc4))\n",
    "        enc4_pool = self.pool(enc4)\n",
    "\n",
    "\n",
    "        # Bottleneck\n",
    "        bottleneck = F.relu(self.bottleneck_conv1(enc4_pool))\n",
    "        bottleneck = F.relu(self.bottleneck_conv2(bottleneck))\n",
    "\n",
    "\n",
    "        # Decoder Block 4\n",
    "        dec4 = self.upconv4(bottleneck)\n",
    "        dec4 = torch.cat([dec4, enc4], dim=1)  # Skip connection\n",
    "        dec4 = F.relu(self.dec4_conv1(dec4))\n",
    "        dec4 = F.relu(self.dec4_conv2(dec4))\n",
    "\n",
    "        # Decoder Block 3\n",
    "        dec3 = self.upconv3(dec4)\n",
    "        dec3 = torch.cat([dec3, enc3], dim=1)  # Skip connection\n",
    "        dec3 = F.relu(self.dec3_conv1(dec3))\n",
    "        dec3 = F.relu(self.dec3_conv2(dec3))\n",
    "\n",
    "        # Decoder Block 2\n",
    "        dec2 = self.upconv2(dec3)\n",
    "        dec2 = torch.cat([dec2, enc2], dim=1)  # Skip connection\n",
    "        dec2 = F.relu(self.dec2_conv1(dec2))\n",
    "        dec2 = F.relu(self.dec2_conv2(dec2))\n",
    "\n",
    "        # Decoder Block 1\n",
    "        dec1 = self.upconv1(dec2)\n",
    "        dec1 = torch.cat([dec1, enc1], dim=1)  # Skip connection\n",
    "        dec1 = F.relu(self.dec1_conv1(dec1))\n",
    "        dec1 = F.relu(self.dec1_conv2(dec1))\n",
    "        out = self.out_conv(dec1)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "OCtcaQ5bnqJL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Found 2913 matching image-mask pairs.\n",
      "183\n"
     ]
    }
   ],
   "source": [
    "#need to convert image data into tensors?\n",
    "device = 'cuda'\n",
    "class PascalVocDataset(Dataset):\n",
    "  def __init__(self, root_dir, annotation_dir, image_transform=None, mask_transform=None):\n",
    "    self.root_dir = root_dir\n",
    "    self.annotation_dir = annotation_dir\n",
    "    self.image_transform = image_transform\n",
    "    self.mask_transform =  mask_transform\n",
    "\n",
    "    img_files = {os.path.splitext(f)[0]: f for f in os.listdir(root_dir) if not f.startswith('.')}\n",
    "    mask_files = {os.path.splitext(f)[0]: f for f in os.listdir(annotation_dir) if not f.startswith('.')}\n",
    "\n",
    "    self.filenames = sorted(list(set(img_files.keys()) & set(mask_files.keys())))\n",
    "    self.img_map = img_files\n",
    "    self.mask_map = mask_files\n",
    "\n",
    "    if len(self.filenames) == 0:\n",
    "        print(f\"Error: 0 pairs found\")\n",
    "        print(f\"Sample images found: {list(img_files.keys())[:5]}\")\n",
    "        print(f\"Sample masks found: {list(mask_files.keys())[:5]}\")\n",
    "    else:\n",
    "        print(f\"Success: Found {len(self.filenames)} matching image-mask pairs.\")\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.filenames)\n",
    "\n",
    "  def __getitem__(self,idx):\n",
    "    fname = self.filenames[idx]\n",
    "   \n",
    "       \n",
    "                \n",
    "    img_path = os.path.join(self.root_dir, self.img_map[fname])\n",
    "    mask_path = os.path.join(self.annotation_dir, self.mask_map[fname])\n",
    "\n",
    "    \n",
    "\n",
    "    image = Image.open(img_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")\n",
    "\n",
    "    if self.image_transform:\n",
    "        image = self.image_transform(image)\n",
    "    if self.mask_transform:\n",
    "        mask = self.mask_transform(mask)\n",
    "\n",
    "    return image, mask\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "\n",
    "root_dir = '/scratch/st-sielmann-1/agrobot/Image_Segmentation/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/JPEGImages'\n",
    "annotation_dir = '/scratch/st-sielmann-1/agrobot/Image_Segmentation/pascal-voc-2012-dataset/VOC2012_train_val/VOC2012_train_val/SegmentationObject'\n",
    "\n",
    "transform_image = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))\n",
    "])\n",
    "transform_annotation = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: ((x * 255) > 0).long().squeeze(0)) #CHANGE #1: Added a squeeze to mask ans binary\n",
    "])\n",
    "\n",
    "\n",
    "customDataset = PascalVocDataset(root_dir, annotation_dir, transform_image, transform_annotation)\n",
    "image_loader = DataLoader(customDataset, batch_size = 16, num_workers=8, shuffle=True,  pin_memory=True, persistent_workers=True)\n",
    "# CHANGE #2: added workers, shuffling, and other params for quicker training\n",
    "\n",
    "model = UNet(input_channels=3, num_classes=2).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001 )\n",
    "scheduler = ReduceLROnPlateau(optimizer, \"min\", patience = 2, factor=0.5)\n",
    "print(len(image_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1/50 | Loss: 0.5885 | Acc: 68.73%\n",
      "Epoch  2/50 | Loss: 0.5887 | Acc: 69.33%\n",
      "Epoch  3/50 | Loss: 0.5269 | Acc: 71.94%\n",
      "Epoch  4/50 | Loss: 0.5014 | Acc: 74.73%\n",
      "Epoch  5/50 | Loss: 0.4891 | Acc: 75.67%\n",
      "Epoch  6/50 | Loss: 0.4824 | Acc: 76.20%\n",
      "Epoch  7/50 | Loss: 0.4682 | Acc: 77.03%\n",
      "Epoch  8/50 | Loss: 0.4562 | Acc: 77.83%\n",
      "Epoch  9/50 | Loss: 0.4576 | Acc: 77.99%\n",
      "Epoch 10/50 | Loss: 0.4498 | Acc: 78.20%\n",
      "Epoch 11/50 | Loss: 0.4402 | Acc: 78.88%\n",
      "Epoch 12/50 | Loss: 0.4344 | Acc: 79.19%\n",
      "Epoch 13/50 | Loss: 0.4307 | Acc: 79.27%\n",
      "Epoch 14/50 | Loss: 0.4279 | Acc: 79.62%\n",
      "Epoch 15/50 | Loss: 0.4227 | Acc: 79.82%\n",
      "Epoch 16/50 | Loss: 0.4153 | Acc: 80.16%\n",
      "Epoch 17/50 | Loss: 0.4187 | Acc: 79.84%\n",
      "Epoch 18/50 | Loss: 0.4071 | Acc: 80.59%\n",
      "Epoch 19/50 | Loss: 0.4017 | Acc: 80.93%\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "epoch_num = 500\n",
    "best_loss = float('inf')\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    num_correct = 0\n",
    "    total_pixels = 0\n",
    "    \n",
    "    for i, (image, mask) in enumerate(image_loader):\n",
    "        image = image.to(device, non_blocking=True)\n",
    "        annotation = mask.to(device, non_blocking=True)\n",
    "\n",
    "        #CHANGE 3: Any time annotations are present for more than one class I squeeze, consistenly of actual use case\n",
    "        if annotation.dim() == 4:\n",
    "            annotation = annotation.squeeze(1)\n",
    "        annotation = annotation.long()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, annotation)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            predicted = torch.argmax(output, dim=1)\n",
    "            num_correct += (predicted == annotation).sum().item()\n",
    "            total_pixels += annotation.numel()\n",
    "        \n",
    "\n",
    "        if i == 5 and epoch % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "                \n",
    "                img_show = (image[0].cpu() * 0.5 + 0.5).permute(1, 2, 0).numpy()\n",
    "                axes[0].imshow(img_show)\n",
    "                axes[0].set_title('Input Image')\n",
    "                \n",
    "                axes[1].imshow(annotation[0].cpu(), cmap='gray')\n",
    "                axes[1].set_title('Ground Truth')\n",
    "                #CHANGE 4: Added an actual model output to the plt, instead save it so training is not interrupted (apparently plt.show() interrupts execution)\n",
    "                axes[2].imshow(predicted[0].cpu(), cmap='gray')\n",
    "                axes[2].set_title(f'Prediction (Epoch {epoch+1})')\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.savefig(f'prediction_epoch_{epoch+1}.png', dpi=100)\n",
    "                plt.close()\n",
    "    \n",
    "    accuracy = 100 * (num_correct / total_pixels)\n",
    "    epoch_loss = running_loss / len(image_loader)\n",
    "    scheduler.step(epoch_loss)\n",
    "    \n",
    "    print(f'Epoch {epoch+1:2d}/{epoch_num} | Loss: {epoch_loss:.4f} | Acc: {accuracy:.2f}%')\n",
    "    #CHANGE 5: saves best weights automatically in case SOCKEYE TIME RUNS OUT OR SOME BUG LIKE KERNEL RESTARTING\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(model.state_dict(), 'best_unet.pth')\n",
    "\n",
    "print(f'\\nTraining complete! Best loss: {best_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "agrobotEnv",
   "language": "python",
   "name": "agrobotenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
